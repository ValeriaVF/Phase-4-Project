{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets by Brand <a class=\"anchor\" id=\"Top\"></a>\n",
    "#### Authors: Eddie Prado, Sally Heinzel, Valeria Viscarra Fossati, and Weston Shuken\n",
    "\n",
    "\n",
    "<img width=\"965\" alt=\"Header Image\" src=\"images/sentiment_analysis_header.png\">\n",
    "\n",
    "###### Image by SurveySensum\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Overview](#Overview)\n",
    "* [Business Opportunity](#Business_Opportunity)\n",
    "* [Data & Methods](#Data_Methods)\n",
    "* [Data Exploration](#Data_Exploration)\n",
    "* [Data Modeling](#Data_Modeling)    \n",
    "    * [Binary Predictor Modeling](#Binary)\n",
    "        * [Baseline Models](#Binary_Baseline)\n",
    "        * [Tuned Models](#Binary_Tuned)\n",
    "    * [Multiclass Predictor Modeling](#Multiclass)\n",
    "        * [Baseline Models](#Multiclass_Baseline)\n",
    "        * [Tuned Models](#Multiclass_Tuned)\n",
    "    * [Added Feature Modeling](#Added_features)\n",
    "    * [Neural Networks](#Neural_Networks)\n",
    "    * [Recurrent Neural Networks](#RNN)\n",
    "* [Final Model](#Final_Model)\n",
    "* [Results & Evaluation](#Results)\n",
    "* [Online Application](#Online_App)\n",
    "* [Recommendations](#Recommendations)\n",
    "* [Next Steps](#Next_Steps)\n",
    "* [Contact Us](#Contact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a class=\"anchor\" id=\"Overview\"></a>\n",
    "Understanding brand and product reputation is difficult when only with provided customer surveys and review data. However, there is an abundance of social media responses to products and brands on various platforms. With these unofficial reviews and preferences towards products via tweets from Twitter, we can derive an overall sentiment towards your brand and products.\n",
    "\n",
    "Our company, ViaGoGo, can provide you with a state-of-the-art machine learning model that rates the product and brand sentiment based on users who tweet about your brand. We can provide you with real-time graphs showing the trends of user sentiment towards your brand and products.\n",
    "\n",
    "This notebook shows the entire process to creating and deploying our ML model for Twitter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Business Opportunity <a class=\"anchor\" id=\"Business_Opportunity\"></a>\n",
    "Companies have little insight into their overall brand reputation on social media platforms. Twitter can provide real-time, accurate analysis of brand reputation based on the sentiment analysis of tweets on Twitter. The word cloud below is an example of how Twitter users are talking about Google and Apple:\n",
    "\n",
    "<img width=\"965\" alt=\"Wordcloud\" src=\"https://user-images.githubusercontent.com/79488205/154748143-17743934-bc8a-40bc-8ec7-86d50fd60665.png\">\n",
    "\n",
    "Our team of experts has built a Machine Learning model that uses Natural Language Processing to distinguish between positive, neutral, and negative sentiment in Tweets. Using Google and Apple mentions on Twitter, we were able to classify sentiment to an 72% accuracy.\n",
    "\n",
    "This model will be used as an analytics tool for companies to access their products' popularity on Twitter without having to access Twitter API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data & Methods <a class=\"anchor\" id=\"Data_Methods\"></a>\n",
    "The dataset comes from Crowdflower via [data.world](https://data.world/crowdflower/brands-and-product-emotions) *Created: August 30, 2013 by Kent Cavender-Bares*. The data contains over 9,000 tweets from Twitter users that evaluated multiple brands and products. The crowd was asked if the tweet expressed positive, negative, or no emotion towards a brand and/or product. If some emotion was expressed, they were also asked to say which brand or product was the target of that emotion. \n",
    "\n",
    "During our exploratory data analysis, we found that the data was not balanced, as shown by the graphs below:\n",
    "\n",
    "#### All data sentiment count:\n",
    "<img width=\"410\" alt=\"all_data_sentiment\" src=\"https://user-images.githubusercontent.com/79488205/154748205-07024099-81f4-4a89-a86f-d5b8271ba599.png\">\n",
    "\n",
    "#### Google & Apple sentiment ratio:\n",
    "<img width=\"411\" alt=\"google_apple_sentiment_ratio\" src=\"https://user-images.githubusercontent.com/79488205/154748394-608d0f0a-7ee0-465c-ab95-cbb2dccea9e3.png\">\n",
    "\n",
    "In order to address this imbalance, we first used a binary classifier to predict if a tweet would have a negative or not negative response. We chose to use this binary classification because negative sentiment is much more insightful to a brand versus neutral or positive sentiment. An example of usage could be to monitor if the negative sentiment increases on a particular day, then we can use inferential analysis to find specific tweets that were affecting the rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis <a class=\"anchor\" id=\"Data_Exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all needed libraries, packages, and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nlp_preprocessing import VIA_GoGo\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "\n",
    "from tensorflow.keras.layers import Dropout, Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "tweet_df = pd.read_csv('data/db_tweet_emotion.csv', encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the dataset\n",
    "tweet_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the datatypes and null values in the data\n",
    "tweet_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning variables of some of the columns for analysis\n",
    "tweet = tweet_df['tweet_text']\n",
    "directed = tweet_df['emotion_in_tweet_is_directed_at']\n",
    "emotion = tweet_df['is_there_an_emotion_directed_at_a_brand_or_product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values in the dataset per column\n",
    "tweet_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of the emotion expressed in the tweet towards the brand or product\n",
    "emotion.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the value counts of the emotion towards the products and brands in the dataset\n",
    "plt.barh(emotion.value_counts().index, emotion.value_counts().values);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts of the tweet towards the brand or product\n",
    "directed.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the value counts of the products and brands in the dataset\n",
    "plt.barh(directed.value_counts().index, directed.value_counts().values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up & Tokenize\n",
    "In this section we will clean up the dataset, tweets, and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating tokenizer\n",
    "tokenizer = RegexpTokenizer(r\"(?u)\\b\\w\\w+\\b\")\n",
    "\n",
    "# Create a list of stopwords in English\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending stop words to the stopwords list of words that appear frequently but don't hold much value\n",
    "sw.append('link')\n",
    "sw.append('rt')\n",
    "sw.append('sxsw')\n",
    "sw.append('quot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating Via_GoGo() -- see viagogo.py for this class\n",
    "vg = VIA_GoGo()\n",
    "\n",
    "# Creating a cleaned and shaped dataframe; This gives new columns 'tweet_text_tokenized', 'joined_tokens', 'emotion_num', & 'brand'. \n",
    "clean_df = vg.clean_tokenizer(tweet_df, tokenizer, sw)\n",
    "clean_df # Viewing the clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making columns that shows either negatvie (1) or not negative (0) sentiment \n",
    "clean_df['emotion_neg_not'] = clean_df['emotion_num']\n",
    "clean_df['emotion_neg_not'] = clean_df['emotion_neg_not'].replace({0:1})\n",
    "clean_df['emotion_neg_not'] = clean_df['emotion_neg_not'].replace({-1:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the imbalance in the binary column\n",
    "clean_df.emotion_neg_not.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to the imbalance in the  multiclass dataset\n",
    "clean_df.emotion_num.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new DataFrame that does not include neutral tweets\n",
    "binary_df = clean_df[clean_df.emotion_num != 0]\n",
    "# Viewing balance of classes when dropped neutral tweets\n",
    "binary_df.emotion_num.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the counts of each emotion type\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "ax = sns.countplot(\n",
    "    data=binary_df, \n",
    "    x=\"directed_at\",\n",
    "    order=binary_df.brand.value_counts().index)\n",
    "\n",
    "ax.set_title('Type of Emotions in Tweets',fontsize='x-large')\n",
    "ax.set_xlabel('Emotion',fontsize='large')\n",
    "ax.set_ylabel('Count',fontsize='large')\n",
    "plt.xticks(range(2), ['Positive','Negative'],fontweight='light', fontsize='large')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the counts of each company discussed in tweets\n",
    "sns.set_theme(style='darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax = sns.countplot(\n",
    "   data=clean_df,\n",
    "   x='brand', order=clean_df['brand'].value_counts().index)\n",
    "ax.set_title('Company Discussed in Tweet', fontsize='x-large')\n",
    "ax.set_xlabel('Comapny/Product',fontsize='large')\n",
    "ax.set_xticklabels(ax.get_xticklabels(),fontweight='light', fontsize='large')\n",
    "ax.set_ylabel('Count',fontsize='large')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize company/product mentioned by positive/non-positive emotion\n",
    "import matplotlib.ticker as mtick\n",
    "clean_df['pos_neg_neutral']=clean_df['emotion_num'] == 1\n",
    "clean_df.groupby(['brand','pos_neg_neutral']).size().groupby(level=0).apply(\n",
    "   lambda x: 100 * x / x.sum()).unstack().plot(kind='bar',stacked=True, figsize=(12,8), legend='reverse')\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter())\n",
    "plt.legend(bbox_to_anchor= (1.02, 1),prop = {'size' : 20},labels=('Not Positive', 'Positive'))\n",
    "plt.xlabel('')\n",
    "plt.xticks(fontsize=16,rotation=0,fontweight='light')\n",
    "plt.yticks(fontsize=16)\n",
    "plt.title('Percentage of Positive vs. Non-Positive Ratings by Company', size=25)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph frequency of top 30 words\n",
    "from sklearn.feature_extraction import text\n",
    "#add custom words to stop word list\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(['sxsw','mention','link','rt'])\n",
    "cv = CountVectorizer(stop_words = stop_words)\n",
    "words = cv.fit_transform(clean_df.joined_tokens)\n",
    "sum_words = words.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
    "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'blue')\n",
    "plt.title('Most Frequently Occuring Words - Top 30', size=20)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A word cloud of the tokenized words\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(background_color = 'white', width = 1000, height = 1000).generate_from_frequencies(dict(words_freq))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.title('WordCloud - Vocabulary from Reviews', fontsize = 22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing top 15 words in only positive emotion tweets\n",
    "frequency_dist_positive = FreqDist(clean_df[\"tweet_text_tokenized\"][clean_df[\"emotion_num\"] == 1].explode())\n",
    "\n",
    "top_ten = list(zip(*frequency_dist_positive.most_common(15)))\n",
    "tokens = top_ten[0]\n",
    "counts = top_ten[1]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(tokens, counts)\n",
    "plt.title('Top 15 Positive Tweet Word Occurences');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing top 15 words in only neutral emotion tweets\n",
    "frequency_dist_neutral = FreqDist(clean_df[\"tweet_text_tokenized\"][clean_df[\"emotion_num\"] == 0].explode())\n",
    "\n",
    "top_ten = list(zip(*frequency_dist_neutral.most_common(15)))\n",
    "tokens = top_ten[0]\n",
    "counts = top_ten[1]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(tokens, counts)\n",
    "plt.title('Top 15 Neutral Tweet Word Occurences');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing top 15 words in only negative emotion tweets\n",
    "frequency_dist_negative = FreqDist(clean_df[\"tweet_text_tokenized\"][clean_df[\"emotion_num\"] == -1].explode())\n",
    "\n",
    "top_ten = list(zip(*frequency_dist_negative.most_common(15)))\n",
    "tokens = top_ten[0]\n",
    "counts = top_ten[1]\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.bar(tokens, counts)\n",
    "plt.title('Top 15 Negative Tweet Word Occurences');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modeling <a class=\"anchor\" id=\"Data_Modeling\"></a>\n",
    "First we will fit our binary prediction models on the negative and not negative predictors.\n",
    "Then we will fit our muliclass prediction models on the positive, negative, and neutral predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Data Predictor <a class=\"anchor\" id=\"Binary\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline  <a class=\"anchor\" id=\"Binary_Baseline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Linear Support Vector Machine\n",
    "sgd = Pipeline([('vect', CountVectorizer()), \n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', SGDClassifier()),\n",
    "              ])\n",
    "\n",
    "# Baseline Naive Bayes\n",
    "naive = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "\n",
    "# Baseline Logistic Regression\n",
    "lin_reg = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', LogisticRegression()),\n",
    "              ])\n",
    "# Baseline Random Forest\n",
    "rfc = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier()),\n",
    "              ])\n",
    "\n",
    "# List of baseline models\n",
    "baseline_models = [sgd, naive, lin_reg, rfc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating empty metric table for attaching scores of model performence.\n",
    "metric_table = pd.DataFrame(columns=['Model', 'CV Score', 'Test Accuracy', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through baseline models and attaching scores to metric table\n",
    "# This run_model() function is part of our ViaGoGo class in viagogo.py\n",
    "for model in baseline_models:   \n",
    "    row = vg.run_model(model, metric_table, clean_df['joined_tokens'], clean_df['emotion_neg_not'], 'binary_baseline', join_str=False)\n",
    "    metric_table = pd.concat([row])\n",
    "\n",
    "metric_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned  <a class=\"anchor\" id=\"Binary_Tuned\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned Linear Support Vector Machine - w/ bigrams\n",
    "sgd_tuned = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer(smooth_idf=False)),\n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "# Tuned Naive Bayes - w/ bigrams\n",
    "naive_tuned = Pipeline([('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "# Tuned Logistic Regression - w/ bigrams\n",
    "lin_reg_tuned = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', LogisticRegression(n_jobs=5)),\n",
    "              ])\n",
    "# Tuned Random Forest - w/ bigrams\n",
    "rfc_tuned = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(max_depth=800, n_estimators=100)),\n",
    "              ])\n",
    "\n",
    "# List of tuned models w/ bigrams\n",
    "baseline_tuned_models = [sgd_tuned, naive_tuned, lin_reg_tuned, rfc_tuned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through tuned models and attaching scores to metric table\n",
    "for model in baseline_tuned_models:   \n",
    "    row = vg.run_model(model, metric_table, clean_df['joined_tokens'], clean_df['emotion_neg_not'], 'binary_tuned', join_str=False)\n",
    "    metric_table = pd.concat([row])\n",
    "\n",
    "metric_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all the binary models\n",
    "vg.plot_models(metric_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Modeling Takeaways:\n",
    "- The accuracy did not improve much over the imbalance in the dataset of 93.7% not negative values.\n",
    "- We will proceed with adding the neutral values back in for multiclass predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Data Predictor  <a class=\"anchor\" id=\"Multiclass\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline  <a class=\"anchor\" id=\"Multiclasss_Baseline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Linear Support Vector Machine \n",
    "sgd_mc = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer(smooth_idf=False)),\n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "# Baseline Naive Bayes\n",
    "naive_mc = Pipeline([('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "# Baseline Logistic Regression\n",
    "lin_reg_mc = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', LogisticRegression(solver='newton-cg', n_jobs=5)),\n",
    "              ])\n",
    "# Baseline Random Forest\n",
    "rfc_mc = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(max_depth=800, n_estimators=100, class_weight='balanced')),\n",
    "              ])\n",
    "\n",
    "# List of multiclass baseline models\n",
    "multiclass_models = [sgd_mc, naive_mc, lin_reg_mc, rfc_mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New metric table for multiclass models\n",
    "metric_table_multi = pd.DataFrame(columns=['Model', 'CV Score', 'Test Accuracy', 'Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through baseline models and attaching scores to metric table\n",
    "for model in multiclass_models:   \n",
    "    row = vg.run_model(model, metric_table_multi, clean_df['joined_tokens'], clean_df['emotion_num'], 'baseline_multiclass', join_str=False)\n",
    "    metric_table_multi = pd.concat([row])\n",
    "\n",
    "metric_table_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned  <a class=\"anchor\" id=\"Multiclass_Tuned\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Linear Support Vector Machine\n",
    "sgd_tuned_mc = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer(smooth_idf=False)),\n",
    "               ('clf', SGDClassifier())\n",
    "              ])\n",
    "# Baseline Naive Bayes\n",
    "naive_tuned_mc = Pipeline([('vect', CountVectorizer(ngram_range=(2,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "# Baseline Logistic Regression\n",
    "lin_reg_tuned_mc = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', LogisticRegression(solver='newton-cg', n_jobs=5)),\n",
    "              ])\n",
    "# Baseline Random Forest\n",
    "rfc_tuned_mc = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(max_depth=800, n_estimators=100, class_weight='balanced')),\n",
    "              ])\n",
    "\n",
    "# List of multiclass tuned models w/ bigrams\n",
    "multiclass_models_tuned = [sgd_tuned_mc, naive_tuned_mc, lin_reg_tuned_mc, rfc_tuned_mc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through tuned models and attaching scores to metric table\n",
    "for model in multiclass_models_tuned:   \n",
    "    row = vg.run_model(model, metric_table_multi, clean_df['joined_tokens'], clean_df['emotion_num'], 'tuned_multiclass', join_str=False)\n",
    "    metric_table_multi = pd.concat([row])\n",
    "\n",
    "metric_table_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all the multiclass models\n",
    "vg.plot_models(metric_table_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Modeling Takeaways:\n",
    "- The accuracy is lower than the binary models which is expected given the imbalance in the dataset. \n",
    "- Our model is performing over 10% better than always predicting the most prevelant class.\n",
    "- Results may vary slightly because we did not specify random state in each classifier.\n",
    "- We will try some more strategies for modeling below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features <a class=\"anchor\" id=\"Added_Features\"></a>\n",
    "In this section, we add some features to out dataset to see if it will improve performence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TextBlob for getting polarity score\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "clean_df['TextBlob_Polarity'] = clean_df['joined_tokens'].apply(getPolarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Afinn to get sentiment score\n",
    "from afinn import Afinn\n",
    "afinn = Afinn(language='en')\n",
    "def getAffinscore(text):\n",
    "    return afinn.score(text)\n",
    "clean_df['Affin_score'] = clean_df['joined_tokens'].apply(getAffinscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing these features\n",
    "sentiment_features = clean_df.loc[:,['TextBlob_Polarity', 'Affin_score']]\n",
    "sentiment_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining these features to the dataframs and vectorizing for modeling\n",
    "clean_df['joined_tokens'] = clean_df['joined_tokens'].str.replace('\\d+', '')\n",
    "\n",
    "X = clean_df['joined_tokens']\n",
    "countvec = CountVectorizer(min_df=5, ngram_range=(1,2), stop_words='english', strip_accents='unicode')\n",
    "\n",
    "\n",
    "X_vec = countvec.fit_transform(X)\n",
    "\n",
    "X_df = pd.DataFrame(X_vec.toarray(), columns=countvec.get_feature_names())\n",
    "preprocess_features = sentiment_features.join(X_df, on=X_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling for new features\n",
    "sgd_feat = SGDClassifier()\n",
    "\n",
    "rfc_feat = RandomForestClassifier(max_depth=100, n_estimators=100, class_weight='balanced')\n",
    "\n",
    "feature_models = [sgd_feat, rfc_feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through new features models and attaching scores to metric table\n",
    "for model in feature_models:   \n",
    "    row = vg.run_model(model, metric_table_multi, preprocess_features, clean_df['emotion_num'], 'features_multiclass', join_str=False, pipeline=False)\n",
    "    metric_table_multi = pd.concat([row])\n",
    "\n",
    "metric_table_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec <a class=\"anchor\" id=\"Word2Vec\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Word2Vec Strategy for improving score -- code shoutout https://github.com/prateekjoshi565/twitter_sentiment_analysis/blob/master/code_sentiment_analysis.ipynb\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            sentences=clean_df.tweet_text_tokenized,\n",
    "            vector_size=200, # desired no. of features/independent variables \n",
    "            window=5, # context window size\n",
    "            min_count=2,\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34)\n",
    "\n",
    "model_w2v.train(clean_df.tweet_text_tokenized, total_examples=len(clean_df.tweet_text_tokenized), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing similarity score to different terms\n",
    "model_w2v.wv.most_similar(positive=\"iphone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.wv.most_similar(positive=\"awesome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for creating vector of the tokens\n",
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary\n",
    "                         \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together into a word2vec dataframe\n",
    "wordvec_arrays = np.zeros((len(clean_df.tweet_text_tokenized), 200))\n",
    "\n",
    "for i in range(len(clean_df.emotion_num)):\n",
    "    wordvec_arrays[i,:] = word_vector(clean_df.tweet_text_tokenized[i], 200)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape  # printing shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling for Word2Vec\n",
    "sgd_word2vec = SGDClassifier()\n",
    "rfc_word2vec = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "word2vec_models = [sgd_word2vec, rfc_word2vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looping through Word2Vec models and attaching scores to metric table\n",
    "for model in word2vec_models:   \n",
    "    row = vg.run_model(model, metric_table_multi, wordvec_df, clean_df['emotion_num'], 'word2vec', join_str=False, pipeline=False)\n",
    "    metric_table_multi = pd.concat([row])\n",
    "    \n",
    "metric_table_multi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network  <a class=\"anchor\" id=\"Neural_Network\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column of targets for NN\n",
    "clean_df['emotion_num_nn'] = clean_df['emotion_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to replace the -1 with 2 for negative sentiment\n",
    "clean_df['emotion_num_nn'] = clean_df['emotion_num_nn'].replace(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating X and y variables\n",
    "X = clean_df['joined_tokens']\n",
    "y =  clean_df['emotion_num_nn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting to test and train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y, \n",
    "                                                    test_size= 0.25,\n",
    "                                                    random_state= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing and transforming with TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "\n",
    "X_train_vec = tfidf.fit_transform(X_train)\n",
    "X_test_vec = tfidf.transform(X_test)\n",
    "\n",
    "# View the shapes of X variables\n",
    "print('Shape of train features tensor:', X_train_vec.shape)\n",
    "print('Shape of test features tensor:', X_test_vec.shape)\n",
    "\n",
    "# View the shapes of y variables\n",
    "print('Shape of train label tensor:', y_train.shape)\n",
    "print('Shape of test label tensor:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Neural Network Model\n",
    "model = Sequential()\n",
    "model.add(layers.Dense(300, input_dim=(38060), activation='relu' ))\n",
    "model.add(Dropout(.4))\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling and running Neural Network\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train_vec, y_train, epochs=6, batch_size=200, validation_split=.20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing test score accuracy\n",
    "results = model.evaluate(X_test_vec, y_test, batch_size=10)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss of train and validation sets by epochs\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy of train and validatio sets by epochs\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending results to metric table\n",
    "NN_dict = {'Model': 'NeuralNetwork', \n",
    "            'CV Score': max(history.history['val_accuracy']),\n",
    "            'Test Accuracy': results[1],\n",
    "            'Type': 'multiclass_NN'}\n",
    "NN_row = pd.DataFrame(NN_dict, index=[0])\n",
    "\n",
    "metric_table_multi = pd.concat([metric_table_multi, NN_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network w/ LSTM <a class=\"anchor\" id=\"RNN\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables for out Recurrent Neural Network -- Code shoutout to Susan Li https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Instantiating Tokenizer and fiting to 'joined_tokens'\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(clean_df['joined_tokens'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing strings and padding sequeneces to the same length -- \"MAX_SEQUENCE_LENGTH\"\n",
    "X = tokenizer.texts_to_sequences(clean_df['joined_tokens'].values) # Transforms each text in texts to a sequence of integers\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape) # Printing shape of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummy variables for sentiment values of 'emotion_num'\n",
    "y = pd.get_dummies(clean_df['emotion_num']).values\n",
    "print('Shape of label tensor:', y.shape) # Printing Y shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting to test and training sets -- printing shapes of each variable\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=.2, random_state=5)\n",
    "print(X_train.shape,Y_train.shape) \n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating Sequential Model \n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(4))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compiling and running Recurrent Neural Network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Defining epochs and batch_size\n",
    "epochs = 6\n",
    "batch_size = 200\n",
    "\n",
    "# Fitting the model\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                                                  patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss of train and validation sets by epochs\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accuracy of train and validatio sets by epochs\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending results to metric table\n",
    "RNN_dict = {'Model': 'RecurrentNeuralNetwork', \n",
    "            'CV Score': max(history.history['val_accuracy']),\n",
    "            'Test Accuracy': 0,\n",
    "            'Type': 'multiclass_NN'}\n",
    "RNN_row = pd.DataFrame(RNN_dict, index=[0])\n",
    "\n",
    "metric_table_multi = pd.concat([metric_table_multi, RNN_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing all the models\n",
    "metric_table_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting all the multiclass models\n",
    "vg.plot_models(metric_table_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling Strategies Takeaways:\n",
    "- With all of the work done, we still have great results from out SGDClassifier.\n",
    "- We will go with this model for the final model, since the accuracy score is high, but also the computing power and time to run is quicker than most other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model <a class=\"anchor\" id=\"Final_Model\"></a>\n",
    "\n",
    "For our final model, we chose to use the top performing on the multiclass in order to predict any of the three classes of tweet sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding our top MULTICLASS MODEL\n",
    "metric_table_multi.sort_values(by=['Test Accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting new variables into train and test sets to evaluate our final model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df['joined_tokens'], \n",
    "                                                    clean_df['emotion_num'], \n",
    "                                                    test_size= 0.25,\n",
    "                                                    random_state= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating variables for the final predictions and final true values\n",
    "final_preds = sgd_tuned_mc.predict(X_test)\n",
    "final_trues = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the confusion matrix for the final model\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_confusion_matrix(sgd_tuned_mc, X_test, y_test, ax=ax, display_labels=['neg', 'neutral', 'pos'])\n",
    "plt.title('Final Model Confusion Matrix')\n",
    "plt.grid(False)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model for usage in our application -- app.py\n",
    "pickle.dump(sgd_tuned_mc, open('final_clf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Evaluation <a class=\"anchor\" id=\"Results\"></a>\n",
    "\n",
    "We built multiclass models to predict whether tweets were positive, neutral and negative. Our baseline model has a 61%  accuracy which is derived by always predicting the neutral class (given its imbalance). We increased this score using machine learning models, specifically Naive Bayes, Random Forest, Neural Network, and SGD Classifier, finally arriving at our best model at ~72% accuracy.\n",
    "\n",
    "<img width=\"405\" alt=\"multiclass_models\" src=\"https://user-images.githubusercontent.com/79488205/154748935-457f5a4e-bf1e-4199-9a80-f0d99913e04d.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Application\n",
    "\n",
    "We created an online application to demo the model's performance. The user can input a sample tweet and click the 'GoGo!' button, which will then return a sentiment score for the sample tweet of Positive, Negative, or Neutral.\n",
    "\n",
    "<img width=\"1200\" alt=\"App Photo\" src=\"images/app_photo.png\">\n",
    "\n",
    "\n",
    "To demo the product, please click on our logo or visit this [link]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations <a class=\"anchor\" id=\"Recommendations\"></a>\n",
    "Being competitive in the 21st century means utilizing 21st-century tools. ViaGogo’s Twitter Sentiment Analysis, built using natural language processing, offers Twitter an opportunity to give their brand users an advantage in the marketplace. This product enables businesses to capture public reactions about their company and products in a far more timely and authentic manner than focus groups or surveys. It collects and analyzes real-time reactions in order for businesses to make effective decisions.\n",
    "\n",
    "The Twitter Sentiment Analysis adds value to businesses in 3 main ways:\n",
    "-\tBrand Perception – track what people are saying about a company/product in real-time within a mercurial social media environment;\n",
    "-\tMarket Research – identify and explore the sentiments directed at one’s competitors in order to develop strategies based on their successes and struggles;\n",
    "-\tCustomer Service – pinpoint which brands, locations, or services are thriving in customer satisfaction and which ones need the most urgent attention.\n",
    "\n",
    "By adopting ViaGogo’s Twitter Sentiment Analysis and offering it as a service for corporate users, Twitter will increase its utility and make itself an indispensable part of the modern business landscape.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps <a class=\"anchor\" id=\"Next_Steps\"></a>\n",
    "ViaGogo in currently working on some new applications for its Twitter Sentiment Analysis. One product in development is the Positivity Rater. This tool gives users a positivity rating based on the analysis of the account’s past tweets. This can be useful to increase engagement among Twitters users. With so many people turned off by negative and harmful content, the Positivity Rater allows an individual to gauge how positive someone is when considering whether or not to follow them. We believe this will boost user activity and retain those who otherwise may stop using Twitter due to so much unwanted, pernicious content.\n",
    "\n",
    "Another area ViaGoGo is actively working on is the creation of dashboards, which allow users to easily visualize sentiment analysis both in real-time and longitudinally. These dashboards will filter data on different dimensions, such as time frame, location, and product/service. It will also facilitate easy comparison of a given metric (for example, comparing the sentiments expressed towards 2 different branches or how sentiment about a product compares to this time last year).\n",
    "\n",
    "Finally, ViaGoGo is already in production of a web app that allows users to write a tweet and see its sentiment rating before publishing it. This will ensure that the author strikes the appropriate tone with their tweet and can prevent thoughtless or poorly constructed tweets from entering the public domain. This feature will be valued by social media managers sitewide.\n",
    "\n",
    "---\n",
    "<h2><center>Brought to you by</center></h2>\n",
    "<center><img width=\"175\" alt=\"Header Image\" align=\"center\" src=\"images/ViaGoGo_logo.png\" ></center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### For more information <a class=\"anchor\" id=\"Contact\"></a>\n",
    "Please contact the contributors to this analysis: \n",
    "[Eddie Prado]() |\n",
    "[Sally Heinzel]() |\n",
    "[Valeria Viscarra Fossati](https://www.linkedin.com/in/valeria-vf/) |\n",
    "[Weston Shuken](https://www.linkedin.com/in/westonshuken/)\n",
    "\n",
    "[Return to top](#Top)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
